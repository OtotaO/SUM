#!/usr/bin/env python3
"""
predictive_intelligence.py - Predictive Intelligence System for SUM

This system transforms SUM from reactive to proactive, anticipating user needs
and providing contextual insights that enhance cognitive capabilities.

Core Components:
1. Context Awareness Engine - Tracks patterns and builds user profiles
2. Proactive Suggestion System - Surfaces relevant insights automatically  
3. Intelligent Scheduling - Optimizes learning and review timing
4. Pattern Recognition - Identifies thinking patterns and blind spots
5. Contextual Knowledge Graph - Maps relationships and connections

Author: ototao
License: Apache License 2.0
"""

import os
import json
import time
import logging
import sqlite3
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union, Set
from dataclasses import dataclass, field, asdict
from pathlib import Path
import threading
from collections import defaultdict, Counter, deque
import re
import math
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('PredictiveIntelligence')


@dataclass
class UserProfile:
    """Comprehensive user preference and behavior profile."""
    user_id: str
    created: datetime = field(default_factory=datetime.now)
    last_updated: datetime = field(default_factory=datetime.now)
    
    # Cognitive patterns
    thinking_style: str = "exploratory"  # exploratory, analytical, creative, systematic
    preferred_complexity: float = 0.5  # 0-1, simple to complex
    attention_span_minutes: int = 25  # Average focus duration
    peak_productivity_hours: List[int] = field(default_factory=list)
    
    # Interest evolution
    primary_interests: List[str] = field(default_factory=list)
    emerging_interests: List[str] = field(default_factory=list)
    declining_interests: List[str] = field(default_factory=list)
    interest_stability: float = 0.7  # How stable interests are over time
    
    # Learning preferences
    preferred_summary_length: int = 100  # words
    likes_examples: bool = True
    prefers_visual_connections: bool = True
    values_historical_context: bool = True
    
    # Behavioral patterns
    capture_frequency: float = 0.0  # thoughts per day
    average_session_length: float = 0.0  # minutes
    cross_referencing_tendency: float = 0.0  # 0-1, how often user connects ideas
    
    # Prediction preferences
    suggestion_sensitivity: float = 0.7  # 0-1, how proactive to be
    interruption_tolerance: float = 0.3  # 0-1, how often to interrupt flow
    insight_threshold: float = 0.6  # minimum confidence for suggestions


@dataclass
class ResearchThread:
    """A coherent line of inquiry the user is pursuing."""
    thread_id: str
    title: str
    started: datetime
    last_activity: datetime
    
    # Thread characteristics
    concepts: List[str] = field(default_factory=list)
    thought_ids: List[str] = field(default_factory=list)
    questions: List[str] = field(default_factory=list)
    insights: List[str] = field(default_factory=list)
    
    # Evolution tracking
    momentum: float = 0.0  # How actively being pursued
    depth_score: float = 0.0  # How deep the exploration has gone
    breadth_score: float = 0.0  # How many related areas explored
    
    # Predictive metadata
    predicted_next_areas: List[str] = field(default_factory=list)
    suggested_connections: List[str] = field(default_factory=list)
    estimated_completion: Optional[datetime] = None


@dataclass
class Insight:
    """A proactive insight generated by the system."""
    insight_id: str
    content: str
    confidence: float
    relevance_score: float
    
    # Context
    triggering_thought_id: Optional[str] = None
    related_thread_id: Optional[str] = None
    insight_type: str = "connection"  # connection, pattern, gap, suggestion
    
    # Timing
    generated: datetime = field(default_factory=datetime.now)
    suggested_delivery: datetime = field(default_factory=datetime.now)
    delivered: Optional[datetime] = None
    
    # User interaction
    user_feedback: Optional[str] = None  # helpful, irrelevant, distracting
    acted_upon: bool = False


class ContextAwarenessEngine:
    """
    Tracks user patterns, research threads, and builds comprehensive understanding
    of user interests and thinking patterns.
    """
    
    def __init__(self, data_dir: str = "predictive_intelligence_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # Storage
        self.db_path = self.data_dir / "predictive_intelligence.db"
        self._init_database()
        
        # Active state
        self.user_profile = UserProfile(user_id="default")
        self.active_threads = {}  # thread_id -> ResearchThread
        self.concept_evolution = defaultdict(list)  # concept -> [(timestamp, frequency)]
        self.session_history = deque(maxlen=100)  # Recent session metadata
        
        # Pattern tracking
        self.thinking_patterns = defaultdict(list)
        self.temporal_patterns = defaultdict(Counter)  # hour -> concept_counts
        self.concept_relationships = defaultdict(lambda: defaultdict(float))
        
        # Load existing data
        self._load_existing_data()
        
        logger.info("Context Awareness Engine initialized")
    
    def _init_database(self):
        """Initialize database for persistent storage."""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS user_profiles (
                    user_id TEXT PRIMARY KEY,
                    profile_data TEXT NOT NULL,
                    created TEXT NOT NULL,
                    last_updated TEXT NOT NULL
                );
                
                CREATE TABLE IF NOT EXISTS research_threads (
                    thread_id TEXT PRIMARY KEY,
                    thread_data TEXT NOT NULL,
                    started TEXT NOT NULL,
                    last_activity TEXT NOT NULL
                );
                
                CREATE TABLE IF NOT EXISTS insights (
                    insight_id TEXT PRIMARY KEY,
                    insight_data TEXT NOT NULL,
                    generated TEXT NOT NULL,
                    delivered TEXT
                );
                
                CREATE TABLE IF NOT EXISTS pattern_events (
                    event_id TEXT PRIMARY KEY,
                    event_type TEXT NOT NULL,
                    event_data TEXT NOT NULL,
                    timestamp TEXT NOT NULL
                );
                
                CREATE INDEX IF NOT EXISTS idx_pattern_events_timestamp 
                ON pattern_events(timestamp);
                CREATE INDEX IF NOT EXISTS idx_insights_generated 
                ON insights(generated);
            """)
    
    def _load_existing_data(self):
        """Load existing data from database."""
        with sqlite3.connect(self.db_path) as conn:
            # Load user profile
            cursor = conn.execute("SELECT * FROM user_profiles WHERE user_id = ?", ("default",))
            row = cursor.fetchone()
            if row:
                profile_data = json.loads(row[1])
                profile_data['created'] = datetime.fromisoformat(profile_data['created'])
                profile_data['last_updated'] = datetime.fromisoformat(profile_data['last_updated'])
                self.user_profile = UserProfile(**profile_data)
            
            # Load research threads
            cursor = conn.execute("SELECT * FROM research_threads ORDER BY last_activity DESC")
            for row in cursor.fetchall():
                thread_data = json.loads(row[1])
                thread_data['started'] = datetime.fromisoformat(thread_data['started'])
                thread_data['last_activity'] = datetime.fromisoformat(thread_data['last_activity'])
                if thread_data.get('estimated_completion'):
                    thread_data['estimated_completion'] = datetime.fromisoformat(thread_data['estimated_completion'])
                
                thread = ResearchThread(**thread_data)
                self.active_threads[thread.thread_id] = thread
    
    def analyze_thought(self, thought_content: str, thought_id: str, timestamp: datetime) -> Dict[str, Any]:
        """Analyze a new thought for patterns and context."""
        analysis = {
            'concepts_extracted': [],
            'threads_updated': [],
            'patterns_detected': [],
            'profile_updates': {},
            'suggested_actions': []
        }
        
        # Extract concepts and themes
        concepts = self._extract_concepts(thought_content)
        analysis['concepts_extracted'] = concepts
        
        # Update concept evolution tracking
        for concept in concepts:
            self.concept_evolution[concept].append((timestamp, 1.0))
        
        # Update or create research threads
        thread_updates = self._update_research_threads(concepts, thought_id, timestamp, thought_content)
        analysis['threads_updated'] = thread_updates
        
        # Detect emerging patterns
        patterns = self._detect_patterns(concepts, timestamp)
        analysis['patterns_detected'] = patterns
        
        # Update user profile
        profile_updates = self._update_user_profile(concepts, timestamp, thought_content)
        analysis['profile_updates'] = profile_updates
        
        # Update temporal patterns
        hour = timestamp.hour
        for concept in concepts:
            self.temporal_patterns[hour][concept] += 1
        
        # Update concept relationships
        self._update_concept_relationships(concepts)
        
        # Persist analysis
        self._save_pattern_event("thought_analyzed", {
            'thought_id': thought_id,
            'concepts': concepts,
            'analysis': analysis
        }, timestamp)
        
        return analysis
    
    def _extract_concepts(self, text: str) -> List[str]:
        """Extract key concepts from text using multiple approaches."""
        concepts = []
        
        # Simple keyword extraction
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        
        # Filter out common words and find important terms
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}
        
        # Find significant words (longer than 3 chars, not stopwords)
        significant_words = [w for w in words if len(w) > 3 and w not in stopwords]
        
        # Find multi-word concepts (simple bigrams)
        text_words = text.lower().split()
        for i in range(len(text_words) - 1):
            bigram = f"{text_words[i]}_{text_words[i+1]}"
            if len(bigram) > 10:  # Reasonable length for concepts
                concepts.append(bigram)
        
        # Add top single words
        word_freq = Counter(significant_words)
        concepts.extend([word for word, freq in word_freq.most_common(5)])
        
        return concepts[:10]  # Limit to top 10 concepts
    
    def _update_research_threads(self, concepts: List[str], thought_id: str, 
                                timestamp: datetime, content: str) -> List[str]:
        """Update existing research threads or create new ones."""
        updated_threads = []
        
        # Find threads with overlapping concepts
        thread_matches = []
        for thread_id, thread in self.active_threads.items():
            overlap = set(concepts) & set(thread.concepts)
            if overlap:
                overlap_score = len(overlap) / max(len(concepts), len(thread.concepts))
                thread_matches.append((thread_id, overlap_score, overlap))
        
        # Update best matching thread
        if thread_matches:
            thread_matches.sort(key=lambda x: x[1], reverse=True)
            best_thread_id, best_score, overlap = thread_matches[0]
            
            if best_score > 0.3:  # Significant overlap
                thread = self.active_threads[best_thread_id]
                thread.thought_ids.append(thought_id)
                thread.last_activity = timestamp
                
                # Update thread concepts with new ones
                new_concepts = set(concepts) - set(thread.concepts)
                thread.concepts.extend(list(new_concepts)[:5])  # Add up to 5 new concepts
                
                # Update momentum
                time_since_last = (timestamp - thread.last_activity).total_seconds() / 3600
                momentum_decay = max(0.1, 1.0 - (time_since_last / 24))  # Decay over 24 hours
                thread.momentum = min(1.0, thread.momentum * momentum_decay + 0.3)
                
                # Detect questions
                if '?' in content:
                    thread.questions.append(content[:200])
                
                updated_threads.append(best_thread_id)
                self._save_research_thread(thread)
        
        # Create new thread if no good match or if concepts suggest new area
        unique_concepts = [c for c in concepts if not any(c in thread.concepts for thread in self.active_threads.values())]
        
        if len(unique_concepts) >= 3 or not thread_matches:
            new_thread = self._create_research_thread(concepts, thought_id, timestamp)
            if new_thread:
                updated_threads.append(new_thread.thread_id)
        
        return updated_threads
    
    def _create_research_thread(self, concepts: List[str], thought_id: str, 
                               timestamp: datetime) -> Optional[ResearchThread]:
        """Create a new research thread."""
        if len(concepts) < 2:
            return None
        
        # Generate thread title from concepts
        title = " & ".join(concepts[:3]).replace('_', ' ').title()
        
        thread = ResearchThread(
            thread_id=f"thread_{int(timestamp.timestamp())}",
            title=title,
            started=timestamp,
            last_activity=timestamp,
            concepts=concepts[:10],
            thought_ids=[thought_id],
            momentum=0.5
        )
        
        self.active_threads[thread.thread_id] = thread
        self._save_research_thread(thread)
        
        logger.info(f"Created new research thread: {title}")
        return thread
    
    def _detect_patterns(self, concepts: List[str], timestamp: datetime) -> List[Dict[str, Any]]:
        """Detect emerging patterns in user thinking."""
        patterns = []
        
        # Concept frequency pattern
        recent_concepts = []
        cutoff = timestamp - timedelta(days=7)
        
        for concept, timeline in self.concept_evolution.items():
            recent_occurrences = [t for t, _ in timeline if t > cutoff]
            if len(recent_occurrences) >= 3:
                patterns.append({
                    'type': 'concept_frequency',
                    'concept': concept,
                    'frequency': len(recent_occurrences),
                    'trend': 'increasing'
                })
        
        # Temporal pattern
        hour = timestamp.hour
        if hour in self.temporal_patterns:
            common_concepts = self.temporal_patterns[hour].most_common(3)
            if any(concept in concepts for concept, _ in common_concepts):
                patterns.append({
                    'type': 'temporal_consistency',
                    'hour': hour,
                    'consistent_concepts': [c for c, _ in common_concepts if c in concepts]
                })
        
        # Connection pattern
        for concept in concepts:
            if concept in self.concept_relationships:
                strong_connections = {
                    related: strength 
                    for related, strength in self.concept_relationships[concept].items()
                    if strength > 0.5
                }
                if strong_connections:
                    patterns.append({
                        'type': 'concept_connection',
                        'concept': concept,
                        'connections': strong_connections
                    })
        
        return patterns
    
    def _update_user_profile(self, concepts: List[str], timestamp: datetime, 
                            content: str) -> Dict[str, Any]:
        """Update user profile based on new thought."""
        updates = {}
        
        # Update thinking style based on content patterns
        if '?' in content:
            # Question indicates exploratory thinking
            updates['thinking_style_hint'] = 'exploratory'
        elif any(word in content.lower() for word in ['analyze', 'examine', 'study', 'research']):
            updates['thinking_style_hint'] = 'analytical'
        elif any(word in content.lower() for word in ['create', 'imagine', 'design', 'invent']):
            updates['thinking_style_hint'] = 'creative'
        
        # Update interests
        current_interests = set(self.user_profile.primary_interests)
        new_concepts = set(concepts) - current_interests
        
        if new_concepts:
            # Add to emerging interests
            self.user_profile.emerging_interests.extend(list(new_concepts)[:3])
            updates['new_emerging_interests'] = list(new_concepts)[:3]
        
        # Update productivity patterns
        hour = timestamp.hour
        if hour not in self.user_profile.peak_productivity_hours:
            # Simple heuristic: if user is actively thinking, it's potentially productive time
            self.user_profile.peak_productivity_hours.append(hour)
            self.user_profile.peak_productivity_hours = sorted(set(self.user_profile.peak_productivity_hours))
            updates['peak_hours_updated'] = True
        
        # Update last activity
        self.user_profile.last_updated = timestamp
        
        # Save profile
        self._save_user_profile()
        
        return updates
    
    def _update_concept_relationships(self, concepts: List[str]):
        """Update the strength of relationships between concepts."""
        for i, concept1 in enumerate(concepts):
            for j, concept2 in enumerate(concepts):
                if i != j:
                    # Increase relationship strength
                    current_strength = self.concept_relationships[concept1][concept2]
                    # Use diminishing returns formula
                    new_strength = current_strength + (1 - current_strength) * 0.1
                    self.concept_relationships[concept1][concept2] = new_strength
    
    def get_active_research_threads(self, limit: int = 10) -> List[ResearchThread]:
        """Get currently active research threads, sorted by momentum."""
        threads = list(self.active_threads.values())
        threads.sort(key=lambda t: t.momentum, reverse=True)
        return threads[:limit]
    
    def get_emerging_interests(self) -> List[str]:
        """Get interests that are emerging based on recent activity."""
        cutoff = datetime.now() - timedelta(days=14)
        
        # Count recent concept occurrences
        recent_concepts = defaultdict(int)
        for concept, timeline in self.concept_evolution.items():
            recent_count = len([t for t, _ in timeline if t > cutoff])
            if recent_count >= 2:  # Minimum threshold
                recent_concepts[concept] = recent_count
        
        # Sort by frequency and novelty
        emerging = []
        for concept, count in recent_concepts.items():
            if concept not in self.user_profile.primary_interests:
                emerging.append((concept, count))
        
        emerging.sort(key=lambda x: x[1], reverse=True)
        return [concept for concept, _ in emerging[:10]]
    
    def predict_next_interests(self, current_concepts: List[str]) -> List[str]:
        """Predict what the user might be interested in next."""
        predictions = []
        
        # Use concept relationships to predict
        for concept in current_concepts:
            if concept in self.concept_relationships:
                related = self.concept_relationships[concept]
                strong_relations = [
                    (related_concept, strength)
                    for related_concept, strength in related.items()
                    if strength > 0.4
                ]
                strong_relations.sort(key=lambda x: x[1], reverse=True)
                predictions.extend([concept for concept, _ in strong_relations[:3]])
        
        # Remove duplicates and current concepts
        predictions = list(set(predictions) - set(current_concepts))
        return predictions[:5]
    
    def _save_user_profile(self):
        """Save user profile to database."""
        with sqlite3.connect(self.db_path) as conn:
            profile_data = asdict(self.user_profile)
            profile_data['created'] = self.user_profile.created.isoformat()
            profile_data['last_updated'] = self.user_profile.last_updated.isoformat()
            
            conn.execute(
                "INSERT OR REPLACE INTO user_profiles (user_id, profile_data, created, last_updated) VALUES (?, ?, ?, ?)",
                (self.user_profile.user_id, json.dumps(profile_data), 
                 self.user_profile.created.isoformat(), self.user_profile.last_updated.isoformat())
            )
    
    def _save_research_thread(self, thread: ResearchThread):
        """Save research thread to database."""
        with sqlite3.connect(self.db_path) as conn:
            thread_data = asdict(thread)
            thread_data['started'] = thread.started.isoformat()
            thread_data['last_activity'] = thread.last_activity.isoformat()
            if thread.estimated_completion:
                thread_data['estimated_completion'] = thread.estimated_completion.isoformat()
            
            conn.execute(
                "INSERT OR REPLACE INTO research_threads (thread_id, thread_data, started, last_activity) VALUES (?, ?, ?, ?)",
                (thread.thread_id, json.dumps(thread_data), 
                 thread.started.isoformat(), thread.last_activity.isoformat())
            )
    
    def _save_pattern_event(self, event_type: str, event_data: Dict[str, Any], timestamp: datetime):
        """Save a pattern event to database."""
        event_id = f"{event_type}_{int(timestamp.timestamp())}_{hashlib.md5(str(event_data).encode()).hexdigest()[:8]}"
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR IGNORE INTO pattern_events (event_id, event_type, event_data, timestamp) VALUES (?, ?, ?, ?)",
                (event_id, event_type, json.dumps(event_data), timestamp.isoformat())
            )
    
    def get_context_summary(self) -> Dict[str, Any]:
        """Get a comprehensive summary of the current context."""
        return {
            'user_profile': {
                'thinking_style': self.user_profile.thinking_style,
                'primary_interests': self.user_profile.primary_interests[:10],
                'emerging_interests': self.get_emerging_interests()[:5],
                'peak_hours': self.user_profile.peak_productivity_hours
            },
            'active_threads': [
                {
                    'title': thread.title,
                    'momentum': thread.momentum,
                    'concepts': thread.concepts[:5],
                    'last_activity': thread.last_activity.isoformat()
                }
                for thread in self.get_active_research_threads(5)
            ],
            'concept_relationships': {
                concept: dict(relationships.most_common(3)) 
                for concept, relationships in Counter(dict(self.concept_relationships)).most_common(10)
            }
        }


class ProactiveSuggestionSystem:
    """
    Generates intelligent, contextual suggestions that surface relevant insights
    from the user's knowledge base before they're explicitly requested.
    """
    
    def __init__(self, context_engine: ContextAwarenessEngine):
        self.context_engine = context_engine
        self.insight_queue = deque(maxlen=50)  # Recent insights waiting to be delivered
        self.delivered_insights = {}  # insight_id -> Insight
        self.suggestion_templates = self._load_suggestion_templates()
        
        # Machine learning components for semantic similarity
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.thought_vectors = None
        self.thought_contents = []
        
        logger.info("Proactive Suggestion System initialized")
    
    def _load_suggestion_templates(self) -> Dict[str, List[str]]:
        """Load templates for different types of suggestions."""
        return {
            'connection': [
                "I noticed you're exploring {current_topic}. This connects beautifully with your earlier thoughts on {related_topic} from {timeframe}.",
                "Your current thinking about {current_topic} reminds me of the insight you had about {related_topic}: '{insight_text}'",
                "There's an interesting parallel between what you're exploring now ({current_topic}) and your previous work on {related_topic}.",
                "This builds nicely on your {timeframe} exploration of {related_topic}. You mentioned: '{insight_text}'"
            ],
            'pattern': [
                "I'm seeing a pattern in your thinking: you often approach {concept} from {angle1} and {angle2}. Have you considered {suggestion}?",
                "Your thoughts about {concept} have evolved over {timeframe}. The progression from {early_thought} to {recent_thought} suggests you might be interested in {suggestion}.",
                "You tend to think most clearly about {concept} during {time_period}. Consider capturing your next thoughts on this topic then."
            ],
            'gap': [
                "You've explored {area1} and {area2} extensively, but there might be an interesting connection through {gap_area}.",
                "Your research thread on {topic} has covered {covered_areas}, but you haven't yet explored {missing_area} - this could unlock new insights.",
                "I notice you often mention {concept} but haven't dove deeply into it. This might be worth a dedicated thinking session."
            ],
            'synthesis': [
                "Your thoughts on {topics} are reaching critical mass. Consider synthesizing them into a unified framework.",
                "You have {count} rich thoughts about {topic}. The patterns suggest they're ready for distillation into key insights.",
                "Your exploration of {topic} over {timeframe} has generated deep insights. Time to weave them together?"
            ]
        }
    
    def generate_suggestions(self, current_thought: str, thought_id: str, 
                           context: Dict[str, Any]) -> List[Insight]:
        """Generate proactive suggestions based on current thought and context."""
        suggestions = []
        
        # Get current context
        active_threads = self.context_engine.get_active_research_threads(5)
        user_profile = self.context_engine.user_profile
        
        # Only generate suggestions if user is receptive
        if user_profile.suggestion_sensitivity < 0.3:
            return suggestions
        
        # Connection suggestions
        connection_insights = self._generate_connection_insights(current_thought, thought_id, active_threads)
        suggestions.extend(connection_insights)
        
        # Pattern suggestions
        pattern_insights = self._generate_pattern_insights(current_thought, context)
        suggestions.extend(pattern_insights)
        
        # Gap identification suggestions
        gap_insights = self._generate_gap_insights(active_threads)
        suggestions.extend(gap_insights)
        
        # Synthesis suggestions
        synthesis_insights = self._generate_synthesis_insights(active_threads)
        suggestions.extend(synthesis_insights)
        
        # Filter and rank suggestions
        filtered_suggestions = self._filter_and_rank_suggestions(suggestions, user_profile)
        
        return filtered_suggestions[:3]  # Return top 3 suggestions
    
    def _generate_connection_insights(self, current_thought: str, thought_id: str, 
                                    active_threads: List[ResearchThread]) -> List[Insight]:
        """Generate insights about connections to previous thoughts."""
        insights = []
        
        current_concepts = self.context_engine._extract_concepts(current_thought)
        
        for thread in active_threads[:3]:  # Check top 3 active threads
            # Find concept overlap
            overlap = set(current_concepts) & set(thread.concepts)
            if not overlap:
                continue
            
            overlap_strength = len(overlap) / max(len(current_concepts), len(thread.concepts))
            if overlap_strength < 0.3:
                continue
            
            # Find specific related insights from thread
            related_insights = [insight for insight in thread.insights if any(concept in insight.lower() for concept in overlap)]
            
            if related_insights or len(thread.thought_ids) >= 3:
                # Generate connection insight
                template = np.random.choice(self.suggestion_templates['connection'])
                
                timeframe = self._format_timeframe(datetime.now() - thread.started)
                related_topic = thread.title.lower()
                current_topic = ', '.join(list(overlap)[:2]).replace('_', ' ')
                
                insight_text = related_insights[0] if related_insights else "your exploration of " + related_topic
                
                content = template.format(
                    current_topic=current_topic,
                    related_topic=related_topic,
                    timeframe=timeframe,
                    insight_text=insight_text[:100] + "..." if len(insight_text) > 100 else insight_text
                )
                
                insight = Insight(
                    insight_id=f"connection_{thought_id}_{thread.thread_id}",
                    content=content,
                    confidence=overlap_strength,
                    relevance_score=thread.momentum * overlap_strength,
                    triggering_thought_id=thought_id,
                    related_thread_id=thread.thread_id,
                    insight_type="connection"
                )
                
                insights.append(insight)
        
        return insights
    
    def _generate_pattern_insights(self, current_thought: str, context: Dict[str, Any]) -> List[Insight]:
        """Generate insights about patterns in thinking."""
        insights = []
        
        patterns = context.get('patterns_detected', [])
        
        for pattern in patterns:
            if pattern['type'] == 'concept_frequency' and pattern['frequency'] >= 4:
                template = np.random.choice(self.suggestion_templates['pattern'])
                
                concept = pattern['concept'].replace('_', ' ')
                
                content = f"I notice you've been thinking about {concept} frequently lately ({pattern['frequency']} times recently). This seems to be a key area of focus for you. Consider dedicating a focused session to explore it more deeply."
                
                insight = Insight(
                    insight_id=f"pattern_{pattern['concept']}_{int(datetime.now().timestamp())}",
                    content=content,
                    confidence=min(pattern['frequency'] / 10.0, 1.0),
                    relevance_score=0.7,
                    insight_type="pattern"
                )
                
                insights.append(insight)
            
            elif pattern['type'] == 'temporal_consistency':
                content = f"You consistently think about {', '.join(pattern['consistent_concepts']).replace('_', ' ')} around {pattern['hour']}:00. Your mind seems naturally drawn to these topics at this time."
                
                insight = Insight(
                    insight_id=f"temporal_{pattern['hour']}_{int(datetime.now().timestamp())}",
                    content=content,
                    confidence=0.8,
                    relevance_score=0.6,
                    insight_type="pattern"
                )
                
                insights.append(insight)
        
        return insights
    
    def _generate_gap_insights(self, active_threads: List[ResearchThread]) -> List[Insight]:
        """Identify potential knowledge gaps and suggest exploration areas."""
        insights = []
        
        if len(active_threads) < 2:
            return insights
        
        # Find potential connections between threads
        thread_concepts = [set(thread.concepts) for thread in active_threads[:3]]
        
        for i, thread1 in enumerate(active_threads[:2]):
            for j, thread2 in enumerate(active_threads[i+1:3], i+1):
                # Check if threads have potential for connection
                concepts1 = set(thread1.concepts)
                concepts2 = set(thread2.concepts)
                
                overlap = concepts1 & concepts2
                if len(overlap) == 0 and len(concepts1) >= 3 and len(concepts2) >= 3:
                    # Find potential bridge concepts
                    bridge_concepts = self._find_bridge_concepts(concepts1, concepts2)
                    
                    if bridge_concepts:
                        template = np.random.choice(self.suggestion_templates['gap'])
                        
                        content = template.format(
                            area1=thread1.title,
                            area2=thread2.title,
                            gap_area=', '.join(bridge_concepts[:2]).replace('_', ' ')
                        )
                        
                        insight = Insight(
                            insight_id=f"gap_{thread1.thread_id}_{thread2.thread_id}",
                            content=content,
                            confidence=0.6,
                            relevance_score=0.5,
                            insight_type="gap"
                        )
                        
                        insights.append(insight)
        
        return insights
    
    def _generate_synthesis_insights(self, active_threads: List[ResearchThread]) -> List[Insight]:
        """Suggest when thoughts are ready for synthesis/densification."""
        insights = []
        
        for thread in active_threads:
            if len(thread.thought_ids) >= 8 or thread.momentum > 0.7:
                # Thread is ready for synthesis
                template = np.random.choice(self.suggestion_templates['synthesis'])
                
                timeframe = self._format_timeframe(datetime.now() - thread.started)
                
                content = template.format(
                    topic=thread.title.lower(),
                    topics=thread.title.lower(),
                    count=len(thread.thought_ids),
                    timeframe=timeframe
                )
                
                insight = Insight(
                    insight_id=f"synthesis_{thread.thread_id}",
                    content=content,
                    confidence=0.8,
                    relevance_score=thread.momentum,
                    related_thread_id=thread.thread_id,
                    insight_type="synthesis"
                )
                
                insights.append(insight)
        
        return insights
    
    def _find_bridge_concepts(self, concepts1: Set[str], concepts2: Set[str]) -> List[str]:
        """Find concepts that could bridge two separate concept sets."""
        # This is a simplified heuristic - in a full implementation, 
        # this would use semantic similarity or knowledge graphs
        
        bridge_concepts = []
        
        # Look for concepts that are semantically related to both sets
        # For now, use simple word similarity
        for concept1 in concepts1:
            for concept2 in concepts2:
                words1 = concept1.split('_')
                words2 = concept2.split('_')
                
                # Find common root words
                for word1 in words1:
                    for word2 in words2:
                        if len(word1) > 3 and len(word2) > 3:
                            if word1[:3] == word2[:3]:  # Common prefix
                                bridge_concepts.append(f"{word1}_{word2}")
        
        return bridge_concepts[:3]
    
    def _filter_and_rank_suggestions(self, suggestions: List[Insight], 
                                   user_profile: UserProfile) -> List[Insight]:
        """Filter and rank suggestions based on user preferences and context."""
        if not suggestions:
            return []
        
        # Filter by confidence threshold
        filtered = [s for s in suggestions if s.confidence >= user_profile.insight_threshold]
        
        # Rank by composite score
        for suggestion in filtered:
            # Composite score considers relevance, confidence, and user preferences
            type_weight = {
                'connection': 1.0,
                'pattern': 0.8,
                'synthesis': 0.9,
                'gap': 0.7
            }.get(suggestion.insight_type, 0.5)
            
            suggestion.relevance_score = (
                suggestion.relevance_score * 0.4 +
                suggestion.confidence * 0.4 +
                type_weight * 0.2
            )
        
        # Sort by relevance score
        filtered.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return filtered
    
    def _format_timeframe(self, timedelta: timedelta) -> str:
        """Format a timedelta into a human-readable timeframe."""
        days = timedelta.days
        hours = timedelta.seconds // 3600
        
        if days == 0:
            if hours == 0:
                return "earlier today"
            elif hours == 1:
                return "an hour ago"
            else:
                return f"{hours} hours ago"
        elif days == 1:
            return "yesterday"
        elif days < 7:
            return f"{days} days ago"
        elif days < 30:
            weeks = days // 7
            return f"{weeks} week{'s' if weeks > 1 else ''} ago"
        else:
            months = days // 30
            return f"{months} month{'s' if months > 1 else ''} ago"
    
    def should_deliver_suggestion(self, insight: Insight, user_profile: UserProfile) -> bool:
        """Determine if and when a suggestion should be delivered to the user."""
        now = datetime.now()
        
        # Check if user is in interruption-tolerant state
        if user_profile.interruption_tolerance < 0.5 and insight.insight_type != 'synthesis':
            # Only deliver high-priority insights if user doesn't like interruptions
            return False
        
        # Check timing - don't interrupt during peak focus times immediately
        current_hour = now.hour
        if current_hour in user_profile.peak_productivity_hours:
            # Wait a bit before suggesting during peak hours
            time_since_generated = (now - insight.generated).total_seconds() / 60
            if time_since_generated < 15:  # Wait 15 minutes
                return False
        
        # Check insight quality
        min_relevance = {
            'connection': 0.6,
            'pattern': 0.7,
            'synthesis': 0.5,
            'gap': 0.8
        }.get(insight.insight_type, 0.7)
        
        return insight.relevance_score >= min_relevance
    
    def deliver_suggestion(self, insight: Insight) -> Dict[str, Any]:
        """Format and deliver a suggestion to the user."""
        insight.delivered = datetime.now()
        self.delivered_insights[insight.insight_id] = insight
        
        return {
            'type': 'proactive_insight',
            'content': insight.content,
            'confidence': insight.confidence,
            'insight_type': insight.insight_type,
            'actions': self._suggest_actions(insight)
        }
    
    def _suggest_actions(self, insight: Insight) -> List[Dict[str, str]]:
        """Suggest concrete actions the user can take based on the insight."""
        actions = []
        
        if insight.insight_type == 'connection':
            actions.extend([
                {'action': 'review_related', 'label': 'Review related thoughts'},
                {'action': 'create_synthesis', 'label': 'Create a synthesis note'},
                {'action': 'expand_connection', 'label': 'Explore this connection further'}
            ])
        
        elif insight.insight_type == 'synthesis':
            actions.extend([
                {'action': 'densify_thread', 'label': 'Compress these thoughts into key insights'},
                {'action': 'create_summary', 'label': 'Create a comprehensive summary'},
                {'action': 'identify_gaps', 'label': 'Identify what\'s missing'}
            ])
        
        elif insight.insight_type == 'pattern':
            actions.extend([
                {'action': 'schedule_focus', 'label': 'Schedule focused thinking time'},
                {'action': 'explore_pattern', 'label': 'Explore this pattern deeper'},
                {'action': 'document_approach', 'label': 'Document your thinking approach'}
            ])
        
        elif insight.insight_type == 'gap':
            actions.extend([
                {'action': 'research_area', 'label': 'Research the suggested area'},
                {'action': 'find_connections', 'label': 'Look for connections'},
                {'action': 'create_bridge', 'label': 'Create bridge notes'}
            ])
        
        return actions[:3]  # Return top 3 actions
    
    def record_user_feedback(self, insight_id: str, feedback: str, acted_upon: bool = False):
        """Record user feedback on delivered insights."""
        if insight_id in self.delivered_insights:
            insight = self.delivered_insights[insight_id]
            insight.user_feedback = feedback
            insight.acted_upon = acted_upon
            
            # Save to database
            self._save_insight(insight)
    
    def _save_insight(self, insight: Insight):
        """Save insight to database."""
        with sqlite3.connect(self.context_engine.db_path) as conn:
            insight_data = asdict(insight)
            insight_data['generated'] = insight.generated.isoformat()
            if insight.suggested_delivery:
                insight_data['suggested_delivery'] = insight.suggested_delivery.isoformat()
            if insight.delivered:
                insight_data['delivered'] = insight.delivered.isoformat()
            
            conn.execute(
                "INSERT OR REPLACE INTO insights (insight_id, insight_data, generated, delivered) VALUES (?, ?, ?, ?)",
                (insight.insight_id, json.dumps(insight_data), 
                 insight.generated.isoformat(), 
                 insight.delivered.isoformat() if insight.delivered else None)
            )


class IntelligentSchedulingEngine:
    """
    Optimizes learning and knowledge review timing using spaced repetition
    and cognitive science principles.
    """
    
    def __init__(self, context_engine: ContextAwarenessEngine):
        self.context_engine = context_engine
        self.review_schedule = {}  # concept -> next_review_time
        self.spaced_repetition_intervals = [1, 3, 7, 14, 30, 90]  # days
        self.scheduled_activities = []  # List of scheduled learning activities
        
        logger.info("Intelligent Scheduling Engine initialized")
    
    def schedule_knowledge_review(self, concept: str, mastery_level: float = 0.5) -> datetime:
        """Schedule next review for a concept based on spaced repetition."""
        now = datetime.now()
        
        # Determine interval based on mastery level
        interval_index = min(int(mastery_level * len(self.spaced_repetition_intervals)), 
                           len(self.spaced_repetition_intervals) - 1)
        interval_days = self.spaced_repetition_intervals[interval_index]
        
        # Add some randomization to avoid clustering
        randomization = np.random.uniform(0.8, 1.2)
        actual_interval = int(interval_days * randomization)
        
        next_review = now + timedelta(days=actual_interval)
        self.review_schedule[concept] = next_review
        
        return next_review
    
    def get_due_reviews(self) -> List[Dict[str, Any]]:
        """Get concepts that are due for review."""
        now = datetime.now()
        due_reviews = []
        
        for concept, next_review in self.review_schedule.items():
            if next_review <= now:
                # Calculate priority based on how overdue and importance
                overdue_hours = (now - next_review).total_seconds() / 3600
                priority = min(overdue_hours / 24, 1.0)  # Max priority of 1.0
                
                due_reviews.append({
                    'concept': concept,
                    'next_review': next_review,
                    'overdue_hours': overdue_hours,
                    'priority': priority
                })
        
        # Sort by priority
        due_reviews.sort(key=lambda x: x['priority'], reverse=True)
        return due_reviews
    
    def suggest_optimal_study_time(self, user_profile: UserProfile) -> List[Dict[str, Any]]:
        """Suggest optimal times for focused study based on user patterns."""
        suggestions = []
        
        # Use peak productivity hours
        for hour in user_profile.peak_productivity_hours:
            suggestions.append({
                'hour': hour,
                'type': 'peak_productivity',
                'confidence': 0.9,
                'duration_minutes': user_profile.attention_span_minutes,
                'description': f"Your mind is typically sharp at {hour}:00"
            })
        
        # Suggest review times before and after peak hours
        for hour in user_profile.peak_productivity_hours:
            if hour > 0:
                suggestions.append({
                    'hour': hour - 1,
                    'type': 'pre_peak_review',
                    'confidence': 0.7,
                    'duration_minutes': 15,
                    'description': f"Quick review before your {hour}:00 peak time"
                })
            
            if hour < 23:
                suggestions.append({
                    'hour': hour + 1,
                    'type': 'post_peak_synthesis',
                    'confidence': 0.6,
                    'duration_minutes': 20,
                    'description': f"Synthesize insights after your {hour}:00 productive period"
                })
        
        return suggestions
    
    def schedule_synthesis_session(self, thread: ResearchThread) -> Dict[str, Any]:
        """Schedule an optimal time for synthesizing a research thread."""
        user_profile = self.context_engine.user_profile
        
        # Find best time based on thread complexity and user patterns
        thread_complexity = len(thread.concepts) + len(thread.thought_ids) / 5
        
        if thread_complexity < 5:
            # Simple synthesis - any productive time works
            suggested_duration = 15
        elif thread_complexity < 15:
            # Medium complexity - need focused time
            suggested_duration = 30
        else:
            # Complex synthesis - need deep focus time
            suggested_duration = 60
        
        # Find optimal time slot
        next_peak_hour = None
        current_hour = datetime.now().hour
        
        for hour in sorted(user_profile.peak_productivity_hours):
            if hour > current_hour:
                next_peak_hour = hour
                break
        
        if not next_peak_hour and user_profile.peak_productivity_hours:
            next_peak_hour = min(user_profile.peak_productivity_hours)
        
        suggested_time = datetime.now().replace(
            hour=next_peak_hour or current_hour + 1,
            minute=0,
            second=0,
            microsecond=0
        )
        
        if suggested_time <= datetime.now():
            suggested_time += timedelta(days=1)
        
        return {
            'thread_id': thread.thread_id,
            'thread_title': thread.title,
            'suggested_time': suggested_time,
            'duration_minutes': suggested_duration,
            'complexity_score': thread_complexity,
            'rationale': f"Best time for {thread.title} synthesis based on your productivity patterns"
        }
    
    def get_learning_curve_optimization(self, concept: str) -> Dict[str, Any]:
        """Analyze learning curve and suggest optimization strategies."""
        # Get concept history
        concept_timeline = self.context_engine.concept_evolution.get(concept, [])
        
        if len(concept_timeline) < 3:
            return {
                'concept': concept,
                'status': 'insufficient_data',
                'recommendation': 'Continue exploring this concept to build learning pattern data'
            }
        
        # Analyze frequency pattern
        timestamps = [t for t, _ in concept_timeline]
        frequencies = [f for _, f in concept_timeline]
        
        # Calculate learning velocity (how quickly understanding is growing)
        time_diffs = [(timestamps[i] - timestamps[i-1]).total_seconds() / 3600 
                      for i in range(1, len(timestamps))]
        avg_time_between = sum(time_diffs) / len(time_diffs) if time_diffs else 24
        
        # Determine learning phase
        recent_activity = len([t for t in timestamps if (datetime.now() - t).days <= 7])
        
        if recent_activity >= 3:
            phase = 'active_exploration'
            recommendation = 'You\'re actively exploring this concept. Consider scheduling a synthesis session soon.'
        elif recent_activity >= 1:
            phase = 'maintenance'
            recommendation = 'Maintain momentum with periodic reviews every few days.'
        else:
            phase = 'dormant'
            recommendation = 'This concept needs revival. Schedule a focused review session.'
        
        return {
            'concept': concept,
            'phase': phase,
            'avg_hours_between_thoughts': avg_time_between,
            'recent_activity_level': recent_activity,
            'recommendation': recommendation,
            'suggested_next_action': self._suggest_next_learning_action(phase, concept)
        }
    
    def _suggest_next_learning_action(self, phase: str, concept: str) -> Dict[str, str]:
        """Suggest the next learning action based on current phase."""
        actions = {
            'active_exploration': {
                'action': 'synthesis',
                'description': f'Create a synthesis document for {concept}',
                'duration': '30-45 minutes'
            },
            'maintenance': {
                'action': 'review_and_expand',
                'description': f'Review {concept} notes and add new perspectives',
                'duration': '15-20 minutes'
            },
            'dormant': {
                'action': 'reactivation',
                'description': f'Revisit {concept} fundamentals and capture current thoughts',
                'duration': '20-30 minutes'
            }
        }
        
        return actions.get(phase, actions['maintenance'])


class PatternRecognitionEngine:
    """
    Identifies thinking patterns, cognitive blind spots, and learning opportunities
    in the user's intellectual journey.
    """
    
    def __init__(self, context_engine: ContextAwarenessEngine):
        self.context_engine = context_engine
        self.cognitive_patterns = defaultdict(list)
        self.blind_spot_indicators = []
        self.thinking_style_evolution = []
        
        logger.info("Pattern Recognition Engine initialized")
    
    def analyze_thinking_patterns(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze patterns in user's thinking across multiple dimensions."""
        if len(thoughts) < 5:
            return {'status': 'insufficient_data'}
        
        patterns = {
            'cognitive_patterns': self._identify_cognitive_patterns(thoughts),
            'temporal_patterns': self._analyze_temporal_patterns(thoughts),
            'complexity_patterns': self._analyze_complexity_evolution(thoughts),
            'questioning_patterns': self._analyze_questioning_behavior(thoughts),
            'connection_patterns': self._analyze_connection_making(thoughts),
            'blind_spots': self._identify_potential_blind_spots(thoughts)
        }
        
        return patterns
    
    def _identify_cognitive_patterns(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Identify recurring cognitive patterns in thinking."""
        patterns = {
            'analytical_tendency': 0.0,
            'creative_tendency': 0.0,
            'systematic_tendency': 0.0,
            'exploratory_tendency': 0.0
        }
        
        analytical_words = ['analyze', 'examine', 'study', 'research', 'investigate', 'evaluate']
        creative_words = ['imagine', 'create', 'design', 'invent', 'innovate', 'brainstorm']
        systematic_words = ['organize', 'structure', 'categorize', 'systematize', 'plan', 'framework']
        exploratory_words = ['explore', 'discover', 'wonder', 'curious', 'investigate', 'question']
        
        total_thoughts = len(thoughts)
        
        for thought in thoughts:
            content = thought.get('content', '').lower()
            
            if any(word in content for word in analytical_words):
                patterns['analytical_tendency'] += 1
            if any(word in content for word in creative_words):
                patterns['creative_tendency'] += 1
            if any(word in content for word in systematic_words):
                patterns['systematic_tendency'] += 1
            if any(word in content for word in exploratory_words):
                patterns['exploratory_tendency'] += 1
        
        # Normalize to percentages
        for key in patterns:
            patterns[key] = patterns[key] / total_thoughts
        
        # Identify dominant pattern
        dominant_pattern = max(patterns.items(), key=lambda x: x[1])
        
        return {
            'tendencies': patterns,
            'dominant_style': dominant_pattern[0].replace('_tendency', ''),
            'style_strength': dominant_pattern[1],
            'is_balanced': max(patterns.values()) - min(patterns.values()) < 0.3
        }
    
    def _analyze_temporal_patterns(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze when and how thinking patterns change over time."""
        if len(thoughts) < 10:
            return {'status': 'insufficient_data'}
        
        # Sort thoughts by timestamp
        sorted_thoughts = sorted(thoughts, key=lambda x: x.get('timestamp', datetime.now()))
        
        # Analyze thinking intensity over time
        daily_counts = defaultdict(int)
        for thought in sorted_thoughts:
            timestamp = thought.get('timestamp', datetime.now())
            if isinstance(timestamp, str):
                timestamp = datetime.fromisoformat(timestamp)
            
            date_key = timestamp.date()
            daily_counts[date_key] += 1
        
        # Find patterns in daily activity
        counts = list(daily_counts.values())
        avg_daily = sum(counts) / len(counts) if counts else 0
        peak_days = [date for date, count in daily_counts.items() if count > avg_daily * 1.5]
        
        # Analyze concept evolution
        concept_timeline = defaultdict(list)
        for thought in sorted_thoughts:
            timestamp = thought.get('timestamp', datetime.now())
            if isinstance(timestamp, str):
                timestamp = datetime.fromisoformat(timestamp)
            
            concepts = thought.get('concepts', [])
            for concept in concepts:
                concept_timeline[concept].append(timestamp)
        
        return {
            'avg_thoughts_per_day': avg_daily,
            'peak_thinking_days': len(peak_days),
            'thinking_consistency': self._calculate_consistency(counts),
            'concept_evolution': {
                concept: {
                    'first_appearance': min(timestamps),
                    'last_appearance': max(timestamps),
                    'frequency': len(timestamps),
                    'lifespan_days': (max(timestamps) - min(timestamps)).days
                }
                for concept, timestamps in concept_timeline.items()
                if len(timestamps) >= 3
            }
        }
    
    def _analyze_complexity_evolution(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze how the complexity of thinking evolves over time."""
        sorted_thoughts = sorted(thoughts, key=lambda x: x.get('timestamp', datetime.now()))
        
        complexity_scores = []
        for thought in sorted_thoughts:
            content = thought.get('content', '')
            concepts = thought.get('concepts', [])
            
            # Simple complexity heuristic
            word_count = len(content.split())
            concept_count = len(concepts)
            question_count = content.count('?')
            
            complexity = (word_count / 10) + (concept_count * 2) + (question_count * 1.5)
            complexity_scores.append(complexity)
        
        if len(complexity_scores) < 5:
            return {'status': 'insufficient_data'}
        
        # Calculate trend
        x = np.arange(len(complexity_scores))
        y = np.array(complexity_scores)
        
        # Simple linear regression
        slope = np.polyfit(x, y, 1)[0]
        
        return {
            'avg_complexity': np.mean(complexity_scores),
            'complexity_trend': 'increasing' if slope > 0.1 else 'decreasing' if slope < -0.1 else 'stable',
            'complexity_variance': np.var(complexity_scores),
            'recent_vs_early_complexity': {
                'recent_avg': np.mean(complexity_scores[-5:]),
                'early_avg': np.mean(complexity_scores[:5]),
                'improvement_ratio': np.mean(complexity_scores[-5:]) / np.mean(complexity_scores[:5]) if np.mean(complexity_scores[:5]) > 0 else 1.0
            }
        }
    
    def _analyze_questioning_behavior(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze patterns in how the user asks questions."""
        question_thoughts = [t for t in thoughts if '?' in t.get('content', '')]
        
        if len(question_thoughts) < 3:
            return {'status': 'insufficient_questions'}
        
        question_types = {
            'what': 0, 'how': 0, 'why': 0, 'when': 0, 'where': 0, 'who': 0,
            'can': 0, 'should': 0, 'would': 0, 'could': 0
        }
        
        total_questions = 0
        for thought in question_thoughts:
            content = thought.get('content', '').lower()
            questions = [s.strip() for s in content.split('?') if s.strip()]
            
            for question in questions:
                total_questions += 1
                for qtype in question_types:
                    if question.startswith(qtype):
                        question_types[qtype] += 1
                        break
        
        # Calculate question frequency
        question_frequency = len(question_thoughts) / len(thoughts)
        
        return {
            'question_frequency': question_frequency,
            'total_questions': total_questions,
            'question_types': question_types,
            'dominant_question_type': max(question_types.items(), key=lambda x: x[1])[0],
            'questioning_style': self._classify_questioning_style(question_types, question_frequency)
        }
    
    def _analyze_connection_making(self, thoughts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze how well the user makes connections between ideas."""
        connection_indicators = ['relate', 'connect', 'similar', 'like', 'reminds', 'builds on', 'links']
        
        connection_thoughts = []
        for thought in thoughts:
            content = thought.get('content', '').lower()
            if any(indicator in content for indicator in connection_indicators):
                connection_thoughts.append(thought)
        
        connection_frequency = len(connection_thoughts) / len(thoughts) if thoughts else 0
        
        # Analyze concept overlap between thoughts
        concept_overlaps = 0
        total_pairs = 0
        
        for i, thought1 in enumerate(thoughts):
            for thought2 in thoughts[i+1:]:
                total_pairs += 1
                concepts1 = set(thought1.get('concepts', []))
                concepts2 = set(thought2.get('concepts', []))
                
                if concepts1 & concepts2:
                    concept_overlaps += 1
        
        conceptual_connectivity = concept_overlaps / total_pairs if total_pairs > 0 else 0
        
        return {
            'explicit_connection_frequency': connection_frequency,
            'conceptual_connectivity': conceptual_connectivity,
            'connection_making_style': 'high' if connection_frequency > 0.3 else 'medium' if connection_frequency > 0.1 else 'low'
        }
    
    def _identify_potential_blind_spots(self, thoughts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify potential cognitive blind spots based on thinking patterns."""
        blind_spots = []
        
        # Analyze concept clustering
        all_concepts = []
        for thought in thoughts:
            all_concepts.extend(thought.get('concepts', []))
        
        concept_counts = Counter(all_concepts)
        
        # Check for over-focus on specific areas
        if concept_counts:
            total_concepts = len(all_concepts)
            most_common = concept_counts.most_common(3)
            
            for concept, count in most_common:
                if count / total_concepts > 0.4:  # Over 40% of all thinking
                    blind_spots.append({
                        'type': 'over_focus',
                        'concept': concept,
                        'frequency': count / total_concepts,
                        'suggestion': f'Consider exploring areas related to but different from {concept}'
                    })
        
        # Check for lack of questioning
        question_frequency = len([t for t in thoughts if '?' in t.get('content', '')]) / len(thoughts)
        if question_frequency < 0.1:
            blind_spots.append({
                'type': 'insufficient_questioning',
                'frequency': question_frequency,
                'suggestion': 'Try asking more questions to deepen your exploration'
            })
        
        # Check for lack of synthesis
        synthesis_indicators = ['summary', 'conclude', 'overall', 'in essence', 'key insight']
        synthesis_frequency = len([
            t for t in thoughts 
            if any(indicator in t.get('content', '').lower() for indicator in synthesis_indicators)
        ]) / len(thoughts)
        
        if synthesis_frequency < 0.05:
            blind_spots.append({
                'type': 'insufficient_synthesis',
                'frequency': synthesis_frequency,
                'suggestion': 'Consider regularly synthesizing your thoughts into key insights'
            })
        
        return blind_spots
    
    def _calculate_consistency(self, values: List[float]) -> float:
        """Calculate consistency score (inverse of coefficient of variation)."""
        if not values or len(values) < 2:
            return 0.0
        
        mean_val = sum(values) / len(values)
        if mean_val == 0:
            return 0.0
        
        variance = sum((x - mean_val) ** 2 for x in values) / len(values)
        std_dev = variance ** 0.5
        
        cv = std_dev / mean_val
        consistency = max(0.0, 1.0 - cv)  # Higher consistency = lower coefficient of variation
        
        return consistency
    
    def _classify_questioning_style(self, question_types: Dict[str, int], frequency: float) -> str:
        """Classify the user's questioning style."""
        if frequency < 0.1:
            return 'low_questioning'
        
        dominant_types = sorted(question_types.items(), key=lambda x: x[1], reverse=True)[:2]
        
        if dominant_types[0][0] in ['why', 'how']:
            return 'analytical_questioner'
        elif dominant_types[0][0] in ['what', 'can', 'could']:
            return 'exploratory_questioner'
        elif dominant_types[0][0] in ['should', 'would']:
            return 'evaluative_questioner'
        else:
            return 'general_questioner'
    
    def suggest_cognitive_improvements(self, patterns: Dict[str, Any]) -> List[Dict[str, str]]:
        """Suggest improvements based on identified patterns."""
        suggestions = []
        
        # Suggestions based on cognitive patterns
        cognitive = patterns.get('cognitive_patterns', {})
        if cognitive.get('is_balanced') == False:
            dominant = cognitive.get('dominant_style', '')
            suggestions.append({
                'type': 'balance_thinking',
                'suggestion': f'Your thinking is heavily {dominant}. Try incorporating more variety in your approach.',
                'action': f'Next time you think about a topic, try a different approach than your usual {dominant} style.'
            })
        
        # Suggestions based on questioning patterns
        questioning = patterns.get('questioning_patterns', {})
        if questioning.get('question_frequency', 0) < 0.15:
            suggestions.append({
                'type': 'increase_questioning',
                'suggestion': 'You could benefit from asking more questions to deepen your exploration.',
                'action': 'Try ending each thinking session with 2-3 questions about what you explored.'
            })
        
        # Suggestions based on blind spots
        blind_spots = patterns.get('blind_spots', [])
        for blind_spot in blind_spots[:2]:  # Top 2 blind spots
            suggestions.append({
                'type': f'address_{blind_spot["type"]}',
                'suggestion': blind_spot['suggestion'],
                'action': f'Focus on this area in your next few thinking sessions.'
            })
        
        return suggestions[:5]  # Return top 5 suggestions


class ContextualKnowledgeGraph:
    """
    Builds and maintains an automated knowledge graph that maps relationships
    between concepts, discovers serendipitous connections, and tracks topic evolution.
    """
    
    def __init__(self, context_engine: ContextAwarenessEngine):
        self.context_engine = context_engine
        self.knowledge_graph = nx.DiGraph()
        self.concept_embeddings = {}  # For semantic similarity
        self.relationship_types = ['relates_to', 'builds_on', 'contradicts', 'exemplifies', 'causes']
        
        logger.info("Contextual Knowledge Graph initialized")
    
    def update_graph(self, thought_content: str, concepts: List[str], thought_id: str):
        """Update the knowledge graph with new thought and concepts."""
        # Add concepts as nodes
        for concept in concepts:
            if not self.knowledge_graph.has_node(concept):
                self.knowledge_graph.add_node(concept, {
                    'first_seen': datetime.now(),
                    'frequency': 0,
                    'related_thoughts': []
                })
            
            # Update node metadata
            node_data = self.knowledge_graph.nodes[concept]
            node_data['frequency'] = node_data.get('frequency', 0) + 1
            node_data['related_thoughts'].append(thought_id)
            node_data['last_seen'] = datetime.now()
        
        # Add edges between co-occurring concepts
        for i, concept1 in enumerate(concepts):
            for concept2 in concepts[i+1:]:
                self._add_or_update_edge(concept1, concept2, 'co_occurs', thought_id)
        
        # Detect semantic relationships
        self._detect_semantic_relationships(thought_content, concepts, thought_id)
        
        # Update concept embeddings
        self._update_concept_embeddings(concepts, thought_content)
    
    def _add_or_update_edge(self, source: str, target: str, relationship: str, thought_id: str):
        """Add or update an edge in the knowledge graph."""
        if self.knowledge_graph.has_edge(source, target):
            edge_data = self.knowledge_graph.edges[source, target]
            edge_data['weight'] = edge_data.get('weight', 0) + 1
            edge_data['supporting_thoughts'].append(thought_id)
        else:
            self.knowledge_graph.add_edge(source, target, {
                'relationship': relationship,
                'weight': 1,
                'created': datetime.now(),
                'supporting_thoughts': [thought_id]
            })
    
    def _detect_semantic_relationships(self, content: str, concepts: List[str], thought_id: str):
        """Detect semantic relationships between concepts from content analysis."""
        content_lower = content.lower()
        
        # Look for causal relationships
        causal_indicators = ['because', 'causes', 'leads to', 'results in', 'due to']
        if any(indicator in content_lower for indicator in causal_indicators):
            # Simple heuristic: concepts appearing before and after causal indicators
            for indicator in causal_indicators:
                if indicator in content_lower:
                    parts = content_lower.split(indicator)
                    if len(parts) >= 2:
                        # Find concepts in before and after parts
                        before_concepts = [c for c in concepts if c in parts[0]]
                        after_concepts = [c for c in concepts if c in parts[1]]
                        
                        for before_concept in before_concepts:
                            for after_concept in after_concepts:
                                self._add_or_update_edge(before_concept, after_concept, 'causes', thought_id)
        
        # Look for contradiction relationships
        contradiction_indicators = ['however', 'but', 'although', 'despite', 'contradicts']
        if any(indicator in content_lower for indicator in contradiction_indicators):
            # Add contradiction relationships between co-occurring concepts
            for i, concept1 in enumerate(concepts):
                for concept2 in concepts[i+1:]:
                    self._add_or_update_edge(concept1, concept2, 'contradicts', thought_id)
        
        # Look for building relationships
        building_indicators = ['builds on', 'extends', 'expands', 'develops']
        if any(indicator in content_lower for indicator in building_indicators):
            for i, concept1 in enumerate(concepts):
                for concept2 in concepts[i+1:]:
                    self._add_or_update_edge(concept1, concept2, 'builds_on', thought_id)
    
    def _update_concept_embeddings(self, concepts: List[str], content: str):
        """Update concept embeddings for semantic similarity calculations."""
        # Simplified embedding using TF-IDF-like approach
        words = content.lower().split()
        
        for concept in concepts:
            if concept not in self.concept_embeddings:
                self.concept_embeddings[concept] = defaultdict(float)
            
            # Update word associations for this concept
            for word in words:
                if len(word) > 3:  # Skip short words
                    self.concept_embeddings[concept][word] += 1.0
    
    def find_unexpected_connections(self, concept: str, depth: int = 2) -> List[Dict[str, Any]]:
        """Find unexpected or serendipitous connections for a given concept."""
        if not self.knowledge_graph.has_node(concept):
            return []
        
        unexpected_connections = []
        
        try:
            # Find concepts at exactly the specified depth
            for target_concept in self.knowledge_graph.nodes():
                if target_concept == concept:
                    continue
                
                try:
                    path = nx.shortest_path(self.knowledge_graph, concept, target_concept)
                    if len(path) - 1 == depth:  # Path length equals depth
                        # Calculate connection strength
                        path_weights = []
                        for i in range(len(path) - 1):
                            if self.knowledge_graph.has_edge(path[i], path[i+1]):
                                weight = self.knowledge_graph.edges[path[i], path[i+1]].get('weight', 1)
                                path_weights.append(weight)
                        
                        connection_strength = min(path_weights) if path_weights else 1
                        
                        # Consider it unexpected if it's a indirect connection with reasonable strength
                        if connection_strength >= 2:  # At least 2 co-occurrences
                            unexpected_connections.append({
                                'target_concept': target_concept,
                                'connection_path': path,
                                'connection_strength': connection_strength,
                                'path_description': self._describe_connection_path(path)
                            })
                
                except nx.NetworkXNoPath:
                    continue
        
        except Exception as e:
            logger.warning(f"Error finding connections for {concept}: {e}")
        
        # Sort by connection strength and return top unexpected connections
        unexpected_connections.sort(key=lambda x: x['connection_strength'], reverse=True)
        return unexpected_connections[:5]
    
    def _describe_connection_path(self, path: List[str]) -> str:
        """Create a human-readable description of a connection path."""
        if len(path) < 2:
            return ""
        
        description_parts = []
        for i in range(len(path) - 1):
            source, target = path[i], path[i+1]
            
            if self.knowledge_graph.has_edge(source, target):
                relationship = self.knowledge_graph.edges[source, target].get('relationship', 'relates_to')
                description_parts.append(f"{source} {relationship} {target}")
            else:
                description_parts.append(f"{source} connects to {target}")
        
        return "  ".join(description_parts)
    
    def get_concept_neighborhoods(self, concept: str, radius: int = 1) -> Dict[str, Any]:
        """Get the neighborhood of concepts around a given concept."""
        if not self.knowledge_graph.has_node(concept):
            return {'concept': concept, 'error': 'concept not found'}
        
        neighborhood = {
            'concept': concept,
            'direct_connections': [],
            'connection_types': defaultdict(list),
            'strongest_connections': [],
            'related_thoughts': []
        }
        
        # Get direct neighbors
        neighbors = list(self.knowledge_graph.neighbors(concept))
        
        for neighbor in neighbors:
            edge_data = self.knowledge_graph.edges[concept, neighbor]
            weight = edge_data.get('weight', 1)
            relationship = edge_data.get('relationship', 'relates_to')
            
            connection_info = {
                'target': neighbor,
                'weight': weight,
                'relationship': relationship,
                'supporting_thoughts': edge_data.get('supporting_thoughts', [])
            }
            
            neighborhood['direct_connections'].append(connection_info)
            neighborhood['connection_types'][relationship].append(neighbor)
        
        # Sort by weight to find strongest connections
        neighborhood['direct_connections'].sort(key=lambda x: x['weight'], reverse=True)
        neighborhood['strongest_connections'] = neighborhood['direct_connections'][:5]
        
        # Get related thoughts
        node_data = self.knowledge_graph.nodes[concept]
        neighborhood['related_thoughts'] = node_data.get('related_thoughts', [])
        neighborhood['frequency'] = node_data.get('frequency', 0)
        
        return neighborhood
    
    def detect_emerging_clusters(self) -> List[Dict[str, Any]]:
        """Detect emerging clusters of related concepts."""
        try:
            # Use community detection to find concept clusters
            communities = nx.community.greedy_modularity_communities(self.knowledge_graph.to_undirected())
            
            clusters = []
            for i, community in enumerate(communities):
                if len(community) >= 3:  # Minimum cluster size
                    community_list = list(community)
                    
                    # Calculate cluster metrics
                    internal_edges = 0
                    total_weight = 0
                    
                    for concept1 in community_list:
                        for concept2 in community_list:
                            if self.knowledge_graph.has_edge(concept1, concept2):
                                internal_edges += 1
                                total_weight += self.knowledge_graph.edges[concept1, concept2].get('weight', 1)
                    
                    cluster_strength = total_weight / max(internal_edges, 1)
                    
                    # Check if cluster is "emerging" (recently active)
                    recent_activity = 0
                    cutoff_date = datetime.now() - timedelta(days=14)
                    
                    for concept in community_list:
                        node_data = self.knowledge_graph.nodes[concept]
                        if node_data.get('last_seen', datetime.min) > cutoff_date:
                            recent_activity += 1
                    
                    emergence_score = recent_activity / len(community_list)
                    
                    if emergence_score > 0.5:  # At least half the concepts are recently active
                        clusters.append({
                            'cluster_id': f'cluster_{i}',
                            'concepts': community_list,
                            'size': len(community_list),
                            'strength': cluster_strength,
                            'emergence_score': emergence_score,
                            'suggested_name': self._suggest_cluster_name(community_list)
                        })
            
            # Sort by emergence score
            clusters.sort(key=lambda x: x['emergence_score'], reverse=True)
            return clusters[:10]  # Return top 10 emerging clusters
        
        except Exception as e:
            logger.warning(f"Error detecting clusters: {e}")
            return []
    
    def _suggest_cluster_name(self, concepts: List[str]) -> str:
        """Suggest a name for a concept cluster."""
        # Simple heuristic: use the most frequent concept as base
        concept_frequencies = {}
        
        for concept in concepts:
            if self.knowledge_graph.has_node(concept):
                freq = self.knowledge_graph.nodes[concept].get('frequency', 0)
                concept_frequencies[concept] = freq
        
        if concept_frequencies:
            primary_concept = max(concept_frequencies.items(), key=lambda x: x[1])[0]
            return f"{primary_concept.replace('_', ' ').title()} Cluster"
        
        return "Unnamed Cluster"
    
    def get_graph_insights(self) -> Dict[str, Any]:
        """Get insights about the overall knowledge graph structure."""
        if self.knowledge_graph.number_of_nodes() == 0:
            return {'status': 'empty_graph'}
        
        insights = {
            'graph_size': {
                'nodes': self.knowledge_graph.number_of_nodes(),
                'edges': self.knowledge_graph.number_of_edges(),
                'density': nx.density(self.knowledge_graph)
            },
            'central_concepts': self._find_central_concepts(),
            'isolated_concepts': self._find_isolated_concepts(),
            'connection_patterns': self._analyze_connection_patterns(),
            'growth_metrics': self._calculate_growth_metrics()
        }
        
        return insights
    
    def _find_central_concepts(self) -> List[Dict[str, Any]]:
        """Find the most central concepts in the knowledge graph."""
        if self.knowledge_graph.number_of_nodes() == 0:
            return []
        
        centrality_metrics = {}
        
        try:
            # Calculate different centrality measures
            degree_centrality = nx.degree_centrality(self.knowledge_graph)
            betweenness_centrality = nx.betweenness_centrality(self.knowledge_graph)
            
            for concept in self.knowledge_graph.nodes():
                centrality_metrics[concept] = {
                    'degree': degree_centrality.get(concept, 0),
                    'betweenness': betweenness_centrality.get(concept, 0),
                    'frequency': self.knowledge_graph.nodes[concept].get('frequency', 0)
                }
        
        except Exception as e:
            logger.warning(f"Error calculating centrality: {e}")
            return []
        
        # Combine metrics for overall centrality score
        central_concepts = []
        for concept, metrics in centrality_metrics.items():
            combined_score = (
                metrics['degree'] * 0.4 +
                metrics['betweenness'] * 0.3 +
                (metrics['frequency'] / 10) * 0.3  # Normalize frequency
            )
            
            central_concepts.append({
                'concept': concept,
                'centrality_score': combined_score,
                'metrics': metrics
            })
        
        central_concepts.sort(key=lambda x: x['centrality_score'], reverse=True)
        return central_concepts[:10]
    
    def _find_isolated_concepts(self) -> List[str]:
        """Find concepts with few or no connections."""
        isolated = []
        
        for concept in self.knowledge_graph.nodes():
            degree = self.knowledge_graph.degree(concept)
            if degree <= 1:  # 0 or 1 connections
                isolated.append(concept)
        
        return isolated
    
    def _analyze_connection_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in how concepts are connected."""
        relationship_counts = defaultdict(int)
        weight_distribution = []
        
        for source, target, edge_data in self.knowledge_graph.edges(data=True):
            relationship = edge_data.get('relationship', 'unknown')
            weight = edge_data.get('weight', 1)
            
            relationship_counts[relationship] += 1
            weight_distribution.append(weight)
        
        return {
            'relationship_types': dict(relationship_counts),
            'avg_connection_strength': sum(weight_distribution) / len(weight_distribution) if weight_distribution else 0,
            'strongest_connections': max(weight_distribution) if weight_distribution else 0,
            'total_relationships': len(weight_distribution)
        }
    
    def _calculate_growth_metrics(self) -> Dict[str, Any]:
        """Calculate how the knowledge graph is growing over time."""
        # This is a simplified version - in practice, you'd track historical data
        recent_nodes = 0
        recent_edges = 0
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for concept in self.knowledge_graph.nodes():
            node_data = self.knowledge_graph.nodes[concept]
            if node_data.get('first_seen', datetime.min) > cutoff_date:
                recent_nodes += 1
        
        for source, target, edge_data in self.knowledge_graph.edges(data=True):
            if edge_data.get('created', datetime.min) > cutoff_date:
                recent_edges += 1
        
        return {
            'recent_concept_growth': recent_nodes,
            'recent_connection_growth': recent_edges,
            'growth_rate': {
                'concepts_per_week': recent_nodes,
                'connections_per_week': recent_edges
            }
        }


class PredictiveIntelligenceSystem:
    """
    Main orchestrator that brings together all predictive intelligence components
    to transform SUM from reactive to proactive knowledge management.
    """
    
    def __init__(self, data_dir: str = "predictive_intelligence_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # Initialize core engines
        self.context_engine = ContextAwarenessEngine(data_dir)
        self.suggestion_system = ProactiveSuggestionSystem(self.context_engine)
        self.scheduling_engine = IntelligentSchedulingEngine(self.context_engine)
        self.pattern_recognition = PatternRecognitionEngine(self.context_engine)
        self.knowledge_graph = ContextualKnowledgeGraph(self.context_engine)
        
        # Integration state
        self.active_insights = deque(maxlen=20)  # Recent insights ready for delivery
        self.notification_queue = deque(maxlen=50)  # Pending notifications
        self.last_analysis_time = datetime.now()
        
        # Background processing
        self._start_background_intelligence()
        
        logger.info("Predictive Intelligence System initialized")
    
    def process_new_thought(self, thought_content: str, thought_id: str) -> Dict[str, Any]:
        """Process a new thought through the full predictive intelligence pipeline."""
        timestamp = datetime.now()
        
        # Step 1: Context analysis
        context_analysis = self.context_engine.analyze_thought(
            thought_content, thought_id, timestamp
        )
        
        # Step 2: Update knowledge graph
        concepts = context_analysis.get('concepts_extracted', [])
        if concepts:
            self.knowledge_graph.update_graph(thought_content, concepts, thought_id)
        
        # Step 3: Generate proactive suggestions
        suggestions = self.suggestion_system.generate_suggestions(
            thought_content, thought_id, context_analysis
        )
        
        # Step 4: Schedule knowledge activities
        scheduling_recommendations = self._generate_scheduling_recommendations(context_analysis)
        
        # Step 5: Queue appropriate insights for delivery
        for suggestion in suggestions:
            if self.suggestion_system.should_deliver_suggestion(
                suggestion, self.context_engine.user_profile
            ):
                self.active_insights.append(suggestion)
        
        # Step 6: Update learning schedules
        for concept in concepts:
            mastery_level = self._estimate_concept_mastery(concept)
            self.scheduling_engine.schedule_knowledge_review(concept, mastery_level)
        
        return {
            'context_analysis': context_analysis,
            'suggestions_generated': len(suggestions),
            'suggestions_queued': len([s for s in suggestions if s in self.active_insights]),
            'scheduling_recommendations': scheduling_recommendations,
            'graph_updated': len(concepts) > 0,
            'new_concepts': concepts
        }
    
    def get_proactive_insights(self, max_insights: int = 3) -> List[Dict[str, Any]]:
        """Get the most relevant proactive insights ready for delivery."""
        insights_to_deliver = []
        
        # Get insights from queue
        while self.active_insights and len(insights_to_deliver) < max_insights:
            insight = self.active_insights.popleft()
            delivered_insight = self.suggestion_system.deliver_suggestion(insight)
            insights_to_deliver.append(delivered_insight)
        
        # Add scheduling recommendations
        due_reviews = self.scheduling_engine.get_due_reviews()
        if due_reviews and len(insights_to_deliver) < max_insights:
            insights_to_deliver.append({
                'type': 'knowledge_review',
                'content': f"You have {len(due_reviews)} concepts ready for review: {', '.join([r['concept'].replace('_', ' ') for r in due_reviews[:3]])}",
                'confidence': 0.8,
                'insight_type': 'scheduling',
                'actions': [
                    {'action': 'review_concepts', 'label': 'Start knowledge review session'},
                    {'action': 'schedule_later', 'label': 'Schedule for later'},
                    {'action': 'adjust_schedule', 'label': 'Adjust review frequency'}
                ]
            })
        
        return insights_to_deliver
    
    def get_contextual_recommendations(self, current_context: str = None) -> Dict[str, Any]:
        """Get contextual recommendations based on current user state."""
        recommendations = {
            'immediate_actions': [],
            'suggested_explorations': [],
            'connection_opportunities': [],
            'learning_optimizations': []
        }
        
        # Get active research threads
        active_threads = self.context_engine.get_active_research_threads(5)
        
        # Immediate actions (synthesis opportunities)
        for thread in active_threads:
            if len(thread.thought_ids) >= 8:
                synthesis_schedule = self.scheduling_engine.schedule_synthesis_session(thread)
                recommendations['immediate_actions'].append({
                    'type': 'synthesis_opportunity',
                    'thread': thread.title,
                    'urgency': 'high' if thread.momentum > 0.7 else 'medium',
                    'estimated_time': synthesis_schedule['duration_minutes'],
                    'description': f"Your thoughts on {thread.title} are ready for synthesis",
                    'best_time': synthesis_schedule['suggested_time'].strftime('%I:%M %p')
                })
        
        # Suggested explorations (knowledge gaps)
        for thread in active_threads[:3]:
            unexpected_connections = self.knowledge_graph.find_unexpected_connections(
                thread.concepts[0] if thread.concepts else thread.title.lower()
            )
            
            for connection in unexpected_connections[:2]:
                recommendations['suggested_explorations'].append({
                    'type': 'exploration_suggestion',
                    'from_thread': thread.title,
                    'suggested_area': connection['target_concept'].replace('_', ' '),
                    'connection_strength': connection['connection_strength'],
                    'description': f"Explore {connection['target_concept'].replace('_', ' ')} - it connects to {thread.title} through {len(connection['connection_path'])} steps"
                })
        
        # Connection opportunities
        emerging_clusters = self.knowledge_graph.detect_emerging_clusters()
        for cluster in emerging_clusters[:2]:
            recommendations['connection_opportunities'].append({
                'type': 'cluster_synthesis',
                'cluster_name': cluster['suggested_name'],
                'concepts': cluster['concepts'][:5],
                'emergence_score': cluster['emergence_score'],
                'description': f"Connect your thinking across {cluster['suggested_name']} - {cluster['size']} related concepts are emerging"
            })
        
        # Learning optimizations
        user_profile = self.context_engine.user_profile
        study_suggestions = self.scheduling_engine.suggest_optimal_study_time(user_profile)
        
        for suggestion in study_suggestions[:3]:
            recommendations['learning_optimizations'].append({
                'type': 'optimal_timing',
                'time': f"{suggestion['hour']}:00",
                'activity_type': suggestion['type'],
                'confidence': suggestion['confidence'],
                'description': suggestion['description']
            })
        
        return recommendations
    
    def analyze_thinking_patterns(self) -> Dict[str, Any]:
        """Analyze user's thinking patterns and provide insights."""
        # Convert thoughts to format expected by pattern recognition
        thoughts_data = []
        
        for thought_id, thought in self.context_engine.active_thoughts.items():
            thoughts_data.append({
                'content': thought.content,
                'concepts': thought.concepts,
                'timestamp': thought.timestamp
            })
        
        # Run pattern analysis
        patterns = self.pattern_recognition.analyze_thinking_patterns(thoughts_data)
        
        if patterns.get('status') == 'insufficient_data':
            return {
                'status': 'insufficient_data',
                'message': 'Continue capturing thoughts to enable pattern analysis',
                'thoughts_needed': max(5 - len(thoughts_data), 0)
            }
        
        # Generate improvement suggestions
        improvements = self.pattern_recognition.suggest_cognitive_improvements(patterns)
        
        # Add graph insights
        graph_insights = self.knowledge_graph.get_graph_insights()
        
        return {
            'thinking_patterns': patterns,
            'improvement_suggestions': improvements,
            'knowledge_graph_insights': graph_insights,
            'overall_assessment': self._generate_overall_assessment(patterns, graph_insights)
        }
    
    def get_intelligent_schedule(self, time_horizon_days: int = 7) -> Dict[str, Any]:
        """Generate an intelligent schedule for the next period."""
        schedule = {
            'daily_recommendations': {},
            'weekly_goals': [],
            'optimal_sessions': []
        }
        
        user_profile = self.context_engine.user_profile
        
        # Generate daily recommendations
        for day_offset in range(time_horizon_days):
            target_date = datetime.now() + timedelta(days=day_offset)
            day_key = target_date.strftime('%Y-%m-%d')
            
            daily_plan = {
                'date': target_date.strftime('%A, %B %d'),
                'peak_hours': user_profile.peak_productivity_hours,
                'scheduled_reviews': [],
                'synthesis_opportunities': [],
                'exploration_suggestions': []
            }
            
            # Add due reviews for this day
            due_reviews = self.scheduling_engine.get_due_reviews()
            reviews_for_day = [r for r in due_reviews if r['next_review'].date() == target_date.date()]
            daily_plan['scheduled_reviews'] = reviews_for_day[:3]  # Max 3 per day
            
            # Add synthesis opportunities
            active_threads = self.context_engine.get_active_research_threads(3)
            for thread in active_threads:
                if thread.momentum > 0.6:  # High momentum threads
                    synthesis_schedule = self.scheduling_engine.schedule_synthesis_session(thread)
                    if synthesis_schedule['suggested_time'].date() == target_date.date():
                        daily_plan['synthesis_opportunities'].append(synthesis_schedule)
            
            schedule['daily_recommendations'][day_key] = daily_plan
        
        # Generate weekly goals
        active_threads = self.context_engine.get_active_research_threads(5)
        for thread in active_threads:
            if thread.momentum > 0.5:
                schedule['weekly_goals'].append({
                    'thread': thread.title,
                    'goal': 'Synthesize insights' if len(thread.thought_ids) >= 8 else 'Deepen exploration',
                    'estimated_sessions': 2 if len(thread.thought_ids) >= 8 else 3,
                    'priority': 'high' if thread.momentum > 0.7 else 'medium'
                })
        
        # Generate optimal learning sessions
        study_times = self.scheduling_engine.suggest_optimal_study_time(user_profile)
        for time_suggestion in study_times:
            schedule['optimal_sessions'].append({
                'time': f"{time_suggestion['hour']}:00",
                'type': time_suggestion['type'],
                'duration': time_suggestion['duration_minutes'],
                'confidence': time_suggestion['confidence'],
                'description': time_suggestion['description']
            })
        
        return schedule
    
    def _generate_scheduling_recommendations(self, context_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate scheduling recommendations based on context analysis."""
        recommendations = []
        
        # Check for concepts that need review
        concepts = context_analysis.get('concepts_extracted', [])
        for concept in concepts:
            learning_curve = self.scheduling_engine.get_learning_curve_optimization(concept)
            if learning_curve.get('phase') == 'active_exploration':
                recommendations.append({
                    'type': 'synthesis_ready',
                    'concept': concept,
                    'recommendation': learning_curve['recommendation'],
                    'suggested_action': learning_curve['suggested_next_action']['description']
                })
        
        # Check for threads ready for synthesis
        updated_threads = context_analysis.get('threads_updated', [])
        for thread_id in updated_threads:
            if thread_id in self.context_engine.active_threads:
                thread = self.context_engine.active_threads[thread_id]
                if len(thread.thought_ids) >= 8:
                    recommendations.append({
                        'type': 'thread_synthesis',
                        'thread': thread.title,
                        'recommendation': f'Your {thread.title} thread has {len(thread.thought_ids)} thoughts and is ready for synthesis'
                    })
        
        return recommendations
    
    def _estimate_concept_mastery(self, concept: str) -> float:
        """Estimate mastery level for a concept based on interaction history."""
        concept_timeline = self.context_engine.concept_evolution.get(concept, [])
        
        if not concept_timeline:
            return 0.1  # New concept
        
        # Simple heuristic based on frequency and recency
        frequency = len(concept_timeline)
        most_recent = max(t for t, _ in concept_timeline)
        days_since_last = (datetime.now() - most_recent).days
        
        # Base mastery on frequency, with decay for time
        base_mastery = min(frequency / 10.0, 0.8)  # Max 0.8 from frequency alone
        time_decay = max(0.1, 1.0 - (days_since_last / 30.0))  # Decay over 30 days
        
        return base_mastery * time_decay
    
    def _generate_overall_assessment(self, patterns: Dict[str, Any], graph_insights: Dict[str, Any]) -> str:
        """Generate a beautiful overall assessment of the user's intellectual journey."""
        if patterns.get('status') == 'insufficient_data':
            return "Your intellectual journey is just beginning. Continue capturing thoughts to unlock deeper insights."
        
        # Extract key metrics
        cognitive = patterns.get('cognitive_patterns', {})
        temporal = patterns.get('temporal_patterns', {})
        graph_size = graph_insights.get('graph_size', {})
        
        dominant_style = cognitive.get('dominant_style', 'balanced')
        nodes_count = graph_size.get('nodes', 0)
        connections_count = graph_size.get('edges', 0)
        
        # Generate personalized assessment
        assessment_parts = []
        
        # Thinking style assessment
        if cognitive.get('is_balanced'):
            assessment_parts.append("Your thinking shows beautiful balance across different cognitive approaches.")
        else:
            assessment_parts.append(f"Your mind gravitates toward {dominant_style} thinking, bringing depth to your explorations.")
        
        # Knowledge network assessment
        if nodes_count > 0:
            density = graph_size.get('density', 0)
            if density > 0.3:
                assessment_parts.append(f"You've built a rich knowledge network of {nodes_count} concepts with {connections_count} connections, showing strong integrative thinking.")
            else:
                assessment_parts.append(f"Your knowledge spans {nodes_count} concepts with room to grow in connecting ideas across domains.")
        
        # Growth trajectory
        complexity_patterns = patterns.get('complexity_patterns', {})
        if complexity_patterns.get('complexity_trend') == 'increasing':
            assessment_parts.append("Your thinking complexity is growing over time, indicating deepening intellectual maturity.")
        
        # Questioning behavior
        questioning = patterns.get('questioning_patterns', {})
        if questioning.get('question_frequency', 0) > 0.2:
            assessment_parts.append("Your curious questioning drives deeper exploration and discovery.")
        
        return " ".join(assessment_parts) or "Your unique thinking patterns are emerging. Continue your intellectual journey to reveal deeper insights."
    
    def _start_background_intelligence(self):
        """Start background processing for continuous intelligence."""
        def background_processor():
            while True:
                try:
                    # Run periodic analysis
                    if (datetime.now() - self.last_analysis_time).total_seconds() > 300:  # Every 5 minutes
                        self._run_periodic_analysis()
                        self.last_analysis_time = datetime.now()
                    
                    time.sleep(60)  # Check every minute
                
                except Exception as e:
                    logger.error(f"Background intelligence error: {e}")
                    time.sleep(300)  # Wait 5 minutes on error
        
        # Start background thread
        background_thread = threading.Thread(target=background_processor, daemon=True)
        background_thread.start()
    
    def _run_periodic_analysis(self):
        """Run periodic analysis to generate insights and maintain system state."""
        # Check for synthesis opportunities
        active_threads = self.context_engine.get_active_research_threads(10)
        
        for thread in active_threads:
            if len(thread.thought_ids) >= 8 and thread.momentum > 0.6:
                # Generate synthesis suggestion
                insight = Insight(
                    insight_id=f"periodic_synthesis_{thread.thread_id}_{int(datetime.now().timestamp())}",
                    content=f"Your exploration of {thread.title} has reached critical mass with {len(thread.thought_ids)} thoughts. The patterns suggest it's time to synthesize these insights into a coherent understanding.",
                    confidence=0.8,
                    relevance_score=thread.momentum,
                    related_thread_id=thread.thread_id,
                    insight_type="synthesis"
                )
                
                if self.suggestion_system.should_deliver_suggestion(insight, self.context_engine.user_profile):
                    self.active_insights.append(insight)
        
        # Check for emerging interest patterns
        emerging_interests = self.context_engine.get_emerging_interests()
        if len(emerging_interests) >= 3:
            # Generate pattern insight
            insight = Insight(
                insight_id=f"emerging_pattern_{int(datetime.now().timestamp())}",
                content=f"I notice you're developing new interests in {', '.join(emerging_interests[:3])}. These emerging themes suggest your intellectual focus is evolving.",
                confidence=0.7,
                relevance_score=0.6,
                insight_type="pattern"
            )
            
            if self.suggestion_system.should_deliver_suggestion(insight, self.context_engine.user_profile):
                self.active_insights.append(insight)
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive status of the predictive intelligence system."""
        return {
            'context_engine': {
                'active_threads': len(self.context_engine.active_threads),
                'concepts_tracked': len(self.context_engine.concept_evolution),
                'user_profile_last_updated': self.context_engine.user_profile.last_updated.isoformat()
            },
            'suggestion_system': {
                'insights_queued': len(self.active_insights),
                'delivered_insights': len(self.suggestion_system.delivered_insights)
            },
            'scheduling_engine': {
                'concepts_scheduled': len(self.scheduling_engine.review_schedule),
                'due_reviews': len(self.scheduling_engine.get_due_reviews())
            },
            'knowledge_graph': self.knowledge_graph.get_graph_insights(),
            'last_analysis': self.last_analysis_time.isoformat(),
            'system_health': 'optimal'
        }


# Beautiful CLI interface for testing the Predictive Intelligence System
def create_predictive_cli():
    """Create a beautiful CLI for testing the predictive intelligence system."""
    print("\n" + "="*70)
    print(" SUM Predictive Intelligence System")
    print("    Transforming reactive to proactive knowledge management")
    print("="*70)
    
    # Initialize the system
    pi_system = PredictiveIntelligenceSystem()
    
    print("\nPredictive Intelligence System initialized!")
    print("Commands: 'think <text>', 'insights', 'patterns', 'schedule', 'graph', 'status', 'quit'")
    
    while True:
        try:
            user_input = input("\n ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("\n Your predictive intelligence journey continues in the background.")
                break
            
            elif user_input.lower().startswith('think '):
                # Process a new thought
                thought_content = user_input[6:]
                thought_id = f"cli_thought_{int(time.time())}"
                
                result = pi_system.process_new_thought(thought_content, thought_id)
                
                print(f"\n Thought processed:")
                print(f"    {result['suggestions_generated']} insights generated")
                print(f"    {len(result['new_concepts'])} concepts identified: {', '.join(result['new_concepts'][:3])}")
                if result['suggestions_queued'] > 0:
                    print(f"    {result['suggestions_queued']} insights ready for delivery")
                
                # Show immediate insights
                insights = pi_system.get_proactive_insights(2)
                for insight in insights:
                    print(f"\n {insight['content']}")
                    if insight.get('actions'):
                        print("   Actions:", ", ".join([a['label'] for a in insight['actions'][:2]]))
            
            elif user_input.lower() == 'insights':
                insights = pi_system.get_proactive_insights(3)
                if insights:
                    print(f"\n {len(insights)} proactive insights:")
                    for i, insight in enumerate(insights, 1):
                        print(f"\n{i}. {insight['content']}")
                        print(f"   Type: {insight['insight_type']} | Confidence: {insight['confidence']:.1%}")
                else:
                    print("\n No new insights right now. Keep thinking to generate more!")
            
            elif user_input.lower() == 'patterns':
                analysis = pi_system.analyze_thinking_patterns()
                if analysis.get('status') == 'insufficient_data':
                    print(f"\n {analysis['message']}")
                    print(f"   Need {analysis['thoughts_needed']} more thoughts for pattern analysis")
                else:
                    print(f"\n Thinking Pattern Analysis:")
                    print(f"   {analysis['overall_assessment']}")
                    
                    if analysis['improvement_suggestions']:
                        print(f"\n Improvement Suggestions:")
                        for suggestion in analysis['improvement_suggestions'][:3]:
                            print(f"    {suggestion['suggestion']}")
            
            elif user_input.lower() == 'schedule':
                schedule = pi_system.get_intelligent_schedule(3)
                print(f"\n Intelligent Schedule (Next 3 Days):")
                
                for date_key, daily_plan in list(schedule['daily_recommendations'].items())[:3]:
                    print(f"\n{daily_plan['date']}:")
                    if daily_plan['scheduled_reviews']:
                        print(f"   Reviews: {', '.join([r['concept'] for r in daily_plan['scheduled_reviews']])}")
                    if daily_plan['synthesis_opportunities']:
                        print(f"   Synthesis: {', '.join([s['thread_title'] for s in daily_plan['synthesis_opportunities']])}")
                
                if schedule['weekly_goals']:
                    print(f"\n Weekly Goals:")
                    for goal in schedule['weekly_goals'][:3]:
                        print(f"    {goal['goal']} for {goal['thread']}")
            
            elif user_input.lower() == 'graph':
                context_summary = pi_system.context_engine.get_context_summary()
                print(f"\n  Knowledge Graph Status:")
                print(f"    {len(context_summary['active_threads'])} active research threads")
                print(f"    {len(context_summary['user_profile']['emerging_interests'])} emerging interests")
                
                if context_summary['active_threads']:
                    print(f"\nActive Threads:")
                    for thread in context_summary['active_threads'][:3]:
                        print(f"    {thread['title']} (momentum: {thread['momentum']:.1%})")
            
            elif user_input.lower() == 'status':
                status = pi_system.get_system_status()
                print(f"\n System Status: {status['system_health'].upper()}")
                print(f"    Context: {status['context_engine']['active_threads']} threads, {status['context_engine']['concepts_tracked']} concepts")
                print(f"    Suggestions: {status['suggestion_system']['insights_queued']} queued, {status['suggestion_system']['delivered_insights']} delivered")
                print(f"    Schedule: {status['scheduling_engine']['due_reviews']} reviews due")
                print(f"    Graph: {status['knowledge_graph']['graph_size']['nodes']} nodes, {status['knowledge_graph']['graph_size']['edges']} edges")
            
            else:
                print("Commands: 'think <text>', 'insights', 'patterns', 'schedule', 'graph', 'status', 'quit'")
        
        except KeyboardInterrupt:
            print("\n\n Your predictive intelligence journey continues in the background.")
            break
        except Exception as e:
            print(f"\n Error: {e}")
            continue


if __name__ == "__main__":
    create_predictive_cli()